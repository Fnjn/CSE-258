{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "\n",
    "import string\n",
    "import math\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "    for l in urllib.request.urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "all_data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "\n",
    "data = all_data[:5000]\n",
    "corpus = [d['review/text'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('with', 'a'), 4587)\n",
      "(('in', 'the'), 2595)\n",
      "(('of', 'the'), 2245)\n",
      "(('is', 'a'), 2056)\n",
      "(('on', 'the'), 2033)\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# 1\n",
    "##########################\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def puncFilter(text):\n",
    "    return ''.join([c for c in text.lower() if c not in punctuation])\n",
    "\n",
    "def text2bigrams(text, rmPunc=True):\n",
    "    if rmPunc:\n",
    "        return nltk.bigrams(puncFilter(text).split())\n",
    "    else:\n",
    "        return nltk.bigrams(text.split())\n",
    "\n",
    "bigrams_cnt = Counter()\n",
    "\n",
    "for text in corpus:\n",
    "    bigrams_cnt += Counter(text2bigrams(text))\n",
    "    \n",
    "for i in bigrams_cnt.most_common(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.342991\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# 2\n",
    "##########################\n",
    "\n",
    "def word_cnt_feature(top_grams, text_grams):\n",
    "    feat = [0] * len(top_grams)\n",
    "    for bi in text_grams:\n",
    "        try:\n",
    "            feat[top_grams.index(bi)] += 1\n",
    "        except:\n",
    "            pass\n",
    "    feat.append(1)\n",
    "    return feat\n",
    "\n",
    "\n",
    "def calMSE(X, y, lamda=1.0):\n",
    "    clf = linear_model.Ridge(lamda, fit_intercept=False)\n",
    "    clf.fit(X, y)\n",
    "    predictions = clf.predict(X)\n",
    "    return np.mean((y - predictions)**2)\n",
    "\n",
    "bigrams = [entry[0] for entry in bigrams_cnt.most_common(1000)]\n",
    "X = [word_cnt_feature(bigrams, text2bigrams(text)) for text in corpus]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "mse = calMSE(X, y)\n",
    "print('MSE: %f' % (mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"foam\" tf-idf score: 2.275737\n",
      "\"smell\" tf-idf score: 0.537902\n",
      "\"banana\" tf-idf score: 3.355561\n",
      "\"lactic\" tf-idf score: 5.841638\n",
      "\"tart\" tf-idf score: 1.806875\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# 3\n",
    "##########################\n",
    "\n",
    "IDF_D = {}\n",
    "\n",
    "def text2unigram(text, rmPunc=True):\n",
    "    if rmPunc:\n",
    "        return puncFilter(text).split()\n",
    "    else:\n",
    "        text.split()\n",
    "\n",
    "def calTF(word, gram):\n",
    "    cnt = Counter(gram)\n",
    "    try:\n",
    "        return cnt[word]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def calIDF(word, cnvt, rmPunc):\n",
    "    try:\n",
    "        idf = IDF_D[word]\n",
    "    except:\n",
    "        numer = len(corpus)\n",
    "        denom = sum([word in cnvt(text, rmPunc) for text in corpus])\n",
    "        denom = denom if denom !=0 else 1e-7 # avoid division by 0\n",
    "        idf = math.log10(numer / denom)\n",
    "        IDF_D[word] = idf\n",
    "    return idf\n",
    "    \n",
    "def calTF_IDF(word, text, cnvt, rmPunc=True):\n",
    "    gram = cnvt(text, rmPunc)\n",
    "    tf = calTF(word, gram)\n",
    "    idf = calIDF(word, cnvt, rmPunc)\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "word_list = ['foam', 'smell', 'banana', 'lactic', 'tart']\n",
    "text = corpus[0]\n",
    "for word in word_list:\n",
    "    tf_idf = calTF_IDF(word, text, text2unigram)\n",
    "    print('\"%s\" tf-idf score: %f' % (word, tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.106130\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# 4\n",
    "##########################\n",
    "\n",
    "def cosine_similarity(x1, x2):\n",
    "    numer = x1.dot(x2)\n",
    "    denom = np.linalg.norm(x1) * np.linalg.norm(x2)\n",
    "    denom = denom if denom !=0 else 1e-7 # avoid division by 0\n",
    "    return numer / denom\n",
    "\n",
    "unigrams_cnt = Counter()\n",
    "\n",
    "for text in corpus:\n",
    "    unigrams_cnt += Counter(text2unigram(text))\n",
    "    \n",
    "unigrams = [entry[0] for entry in unigrams_cnt.most_common(1000)]\n",
    "\n",
    "text1 = corpus[0]\n",
    "text2 = corpus[1]\n",
    "\n",
    "tf_idf1 = np.array([calTF_IDF(w, text1, text2unigram) for w in unigrams])\n",
    "tf_idf2 = np.array([calTF_IDF(w, text2, text2unigram) for w in unigrams])\n",
    "\n",
    "cos_sim = cosine_similarity(tf_idf1, tf_idf2)\n",
    "print('Cosine similarity: %f' %(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# 5\n",
    "##########################\n",
    "\n",
    "tf_idf_res = {}\n",
    "for i, text in enumerate(corpus):\n",
    "    tf_idf_res[i] = np.array([calTF_IDF(w, text, text2unigram) for w in unigrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fanjin/.local/lib/python3.5/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "cos_sim_res = {}\n",
    "for k, v in tf_idf_res.items():\n",
    "    cos_sim_res[k] = cosine_similarity(tf_idf_res[0], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.0), (4003, 0.3173276600263313)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cos_sim_res.items(), key=lambda x: x[1], reverse=True)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "5000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d019b32b204f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTF_IDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf_idf_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review/overall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d019b32b204f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTF_IDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf_idf_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review/overall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d019b32b204f>\u001b[0m in \u001b[0;36mtf_idf_feature\u001b[0;34m(TF_IDF, i)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtf_idf_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTF_IDF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTF_IDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf_idf_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 5000"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# 6\n",
    "##########################\n",
    "\n",
    "def tf_idf_feature(TF_IDF,i):\n",
    "    return TF_IDF[i]\n",
    "\n",
    "X = [tf_idf_feature(tf_idf_res, i) for i in range(len(data))]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "mse = calMSE(X, y)\n",
    "print('MSE: %f' % (mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# 7\n",
    "##########################\n",
    "\n",
    "data = all_data\n",
    "corpus = [d['review/text'] for d in data]\n",
    "\n",
    "def split_data(X, Y, n_train, n_val, n_test, shuffle=False):\n",
    "    m = len(X)\n",
    "    n_val += n_train\n",
    "    n_test += n_val\n",
    "\n",
    "    if shuffle:\n",
    "        r = list(zip(X, Y))\n",
    "        random.shuffle(r)\n",
    "        X, Y = list(zip(*r))\n",
    "\n",
    "    return (X[:n_train], Y[:n_train]), (X[n_train:n_val], Y[n_train:n_val]), \\\n",
    "            (X[n_val:n_test], Y[n_val:n_test])\n",
    "\n",
    "# X = [feature(text) for text in corpus]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "trainData, valData, testData = split_data(corpus, y, 5000, 5000, 5000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, top_grams, corpus, cnvt, rmPunc, use_tf_idf, TF_IDF=None):\n",
    "        self.top_grams = top_grams\n",
    "        self.corpus = corpus\n",
    "        self.cnvt = cnvt\n",
    "        self.rmPunc = rmPunc\n",
    "        self.use_tf_idf = use_tf_idf\n",
    "        \n",
    "        self.TF_IDF = TF_IDF\n",
    "        \n",
    "    def get_feature(self, corpus):\n",
    "        if self.use_tf_idf:\n",
    "            return [tf_idf_feature(self.TF_IDF, i) for i in range(len(corpus))]\n",
    "        else:\n",
    "            return [word_cnt_feature(self.top_grams, self.cnvt(text, self.rmPunc)) \n",
    "                    for text in corpus]\n",
    "    \n",
    "    def train(self, X, y, lamda):\n",
    "        self.clf = linear_model.Ridge(lamda, fit_intercept=False)\n",
    "        self.clf.fit(X, y)\n",
    "    \n",
    "    def test(self, X, y=None):\n",
    "        predictions = clf.predict(X)\n",
    "        if y is not None:\n",
    "            return np.mean((y - predictions)**2)\n",
    "        else:\n",
    "            return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = [0.01, 0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF1 = None\n",
    "TF_IDF2 = None\n",
    "TF_IDF3 = None\n",
    "TF_IDF4 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCorpus, trainY = trainData\n",
    "\n",
    "unigrams_cnt = Counter()\n",
    "bigrams_cnt = Counter()\n",
    "\n",
    "for text in trainCorpus:\n",
    "    unigrams_cnt += Counter(text2unigram(text))\n",
    "    bigrams_cnt += Counter(text2bigrams(text))\n",
    "    \n",
    "unigrams = [entry[0] for entry in unigrams_cnt.most_common(1000)]\n",
    "bigrams = [entry[0] for entry in bigrams_cnt.most_common(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = Model(unigrams, trainCorpus, text2unigram, rmPunc=True, use_tf_idf=True, TF_IDF=TF_IDF1)\n",
    "m2 = Model(bigrams, trainCorpus, text2bigrams, rmPunc=True, use_tf_idf=True, TF_IDF=TF_IDF2)\n",
    "\n",
    "m3 = Model(unigrams, trainCorpus, text2unigram, rmPunc=False, use_tf_idf=True, TF_IDF=TF_IDF3)\n",
    "m4 = Model(bigrams, trainCorpus, text2bigrams, rmPunc=False, use_tf_idf=True, TF_IDF=TF_IDF4)\n",
    "\n",
    "m5 = Model(unigrams, trainCorpus, text2unigram, rmPunc=True, use_tf_idf=False)\n",
    "m6 = Model(bigrams, trainCorpus, text2bigrams, rmPunc=True, use_tf_idf=False)\n",
    "\n",
    "m7 = Model(unigrams, trainCorpus, text2unigram, rmPunc=False, use_tf_idf=False)\n",
    "m8 = Model(bigrams, trainCorpus, text2bigrams, rmPunc=False, use_tf_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
