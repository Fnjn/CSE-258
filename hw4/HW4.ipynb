{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "    for l in urllib.request.urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "all_data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "\n",
    "data = all_data[:5000]\n",
    "corpus = [d['review/text'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = data[0]['review/text']\n",
    "# t1 = text2bigrams(data[0]['review/text'])\n",
    "# t2 = text2bigrams(data[1]['review/text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('with', 'a'), 4587), (('in', 'the'), 2595), (('of', 'the'), 2245), (('is', 'a'), 2056), (('on', 'the'), 2033)]\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# 1\n",
    "##########################\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def puncFilter(text):\n",
    "    return ''.join([c for c in text.lower() if c not in punctuation])\n",
    "\n",
    "def text2bigrams(text):\n",
    "    return nltk.bigrams(puncFilter(text).split())\n",
    "\n",
    "from collections import Counter\n",
    "bigrams_cnt = Counter()\n",
    "\n",
    "for text in corpus:\n",
    "    bigrams_cnt += Counter(text2bigrams(text))\n",
    "    \n",
    "print(bigrams_cnt.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3435151332735128"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################\n",
    "# 2\n",
    "##########################\n",
    "\n",
    "def feature(text):\n",
    "    feat = [0] * len(bigrams)\n",
    "    r = text2bigrams(text)\n",
    "    for bi in r:\n",
    "        try:\n",
    "            feat[bigrams.index(bi)] += 1\n",
    "        except:\n",
    "            pass\n",
    "    feat.append(1)\n",
    "    return feat\n",
    "\n",
    "bigrams = [entry[0] for entry in bigrams_cnt.most_common(1000)]\n",
    "X = [feature(text) for text in corpus]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "predictions = clf.predict(X)\n",
    "mse = np.mean((y - predictions)**2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"foam\" tf-idf score: 2.275737\n",
      "\"smell\" tf-idf score: 0.537902\n",
      "\"banana\" tf-idf score: 3.355561\n",
      "\"lactic\" tf-idf score: 5.841638\n",
      "\"tart\" tf-idf score: 1.806875\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# 3\n",
    "##########################\n",
    "\n",
    "def text2unigram(text):\n",
    "    return puncFilter(text).split()\n",
    "\n",
    "def calTF(word, text, cnvt):\n",
    "    cnt = Counter(cnvt(text))\n",
    "    try:\n",
    "        return cnt[word]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "import math\n",
    "def calIDF(word, cnvt):\n",
    "    numer = len(corpus)\n",
    "    denom = sum([word in cnvt(text) for text in corpus]) \n",
    "    return math.log10(numer / denom)\n",
    "    \n",
    "def calTF_IDF(word, text, cnvt):\n",
    "    tf = calTF(word, text, cnvt)\n",
    "    idf = calIDF(word, cnvt)\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "word_list = ['foam', 'smell', 'banana', 'lactic', 'tart']\n",
    "text = corpus[0]\n",
    "for word in word_list:\n",
    "    tf_idf = calTF_IDF(word, text, text2unigram)\n",
    "    print('\"%s\" tf-idf score: %f' % (word, tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# 4\n",
    "##########################\n",
    "\n",
    "text1 = corpus[0]\n",
    "text2 = corpus[1]\n",
    "\n",
    "bigram1 = text2bigrams(text1)\n",
    "bigram2 = text2bigrams(text2)\n",
    "\n",
    "tf_idf1 = {w:calTF_IDF(w, text1, text2bigrams) for w in bigram1}\n",
    "tf_idf2 = {w:calTF_IDF(w, text2, text2bigrams) for w in bigram2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# 5\n",
    "##########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# 6\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000 6000 6000 7000 7000\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# 7\n",
    "##########################\n",
    "\n",
    "data = all_data\n",
    "corpus = [d['review/text'] for d in data]\n",
    "\n",
    "def split_data(X, Y, n_train, n_val, n_test, shuffle=False):\n",
    "    m = len(X)\n",
    "    n_val += n_train\n",
    "    n_test += n_val\n",
    "\n",
    "    if shuffle:\n",
    "        r = list(zip(X, Y))\n",
    "        random.shuffle(r)\n",
    "        X, Y = list(zip(*r))\n",
    "\n",
    "    return X[:n_train], Y[:n_train], X[n_train:n_val], \\\n",
    "        Y[n_train:n_val], X[n_val:n_test], Y[n_val:n_test]\n",
    "\n",
    "X = [feature(text) for text in corpus]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "trainX, trainY, valX, valY, testX, testY = split_data(X, y, 5000, 5000, 5000, shuffle=True)\n",
    "# print(len(trainX), len(trainY), len(valX), len(valY), len(testX), len(testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = [0.01, 0.1, 1, 10, 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
